{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsFXsLLRTgkDXmkQ5OjEmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanzaniav0825/CS667/blob/main/Project_1_deliverables_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "SERPAPI_KEY = getpass(\"Enter your SerpAPI key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brH-Q0PyWBqr",
        "outputId": "0b74fbf3-198f-4aec-ea63-c9ec86ad3ea3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your SerpAPI key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tldextract\n",
        "!pip install joblib\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSpMQqYdWTya",
        "outputId": "3ea272c4-7746-4ec5-8b28-347fe8c03c6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (5.1.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.32.3)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract) (3.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract) (2025.1.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taMX5B4he2ia",
        "outputId": "32a853c9-631f-4bd0-9780-93bb8cf98782"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.1.31)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=d136ec3d795a496165cd36d365d4c85df9e0adad824accfd8370bc664a021e18\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tldextract\n",
        "import re\n",
        "import random\n",
        "from serpapi import GoogleSearch\n",
        "\n",
        "class CredibilityScorer:\n",
        "    def __init__(self, url, serpapi_key=None):\n",
        "        self.url = url\n",
        "        self.serpapi_key = serpapi_key\n",
        "        self.domain_info = tldextract.extract(url)\n",
        "        self.domain = f\"{self.domain_info.domain}.{self.domain_info.suffix}\"\n",
        "        self.score = 0.0\n",
        "        self.explanation_parts = []\n",
        "\n",
        "    def domain_suffix_score(self):\n",
        "        suffix = self.domain_info.suffix\n",
        "        if suffix in [\"edu\", \"gov\"]:\n",
        "            self.score += 0.3\n",
        "            self.explanation_parts.append(f\"‚úÖ Domain suffix '.{suffix}' indicates high credibility (academic/government).\")\n",
        "        elif suffix == \"org\":\n",
        "            self.score += 0.2\n",
        "            self.explanation_parts.append(\"üü° '.org' is often non-profit, moderately credible.\")\n",
        "        else:\n",
        "            self.score += 0.1\n",
        "            self.explanation_parts.append(f\"‚ö†Ô∏è '.{suffix}' is a common/commercial domain ‚Äî minimal trust boost.\")\n",
        "\n",
        "    def trusted_domain_score(self):\n",
        "        trusted_sources = [\"nature.com\", \"sciencedirect.com\", \"springer.com\", \"jstor.org\", \"arxiv.org\"]\n",
        "        if any(source in self.url for source in trusted_sources):\n",
        "            self.score += 0.4\n",
        "            self.explanation_parts.append(f\"‚úÖ Trusted academic publisher detected: `{self.domain}`.\")\n",
        "        else:\n",
        "            self.explanation_parts.append(f\"üîç `{self.domain}` not on the trusted source list.\")\n",
        "\n",
        "    def recent_year_score(self):\n",
        "        if re.search(r\"/20\\d{2}/\", self.url):\n",
        "            self.score += 0.1\n",
        "            self.explanation_parts.append(\"üìÖ Recent year found in URL ‚Äî likely up-to-date.\")\n",
        "        else:\n",
        "            self.explanation_parts.append(\"üìÖ No recent date detected in URL.\")\n",
        "\n",
        "    def https_check(self):\n",
        "        if self.url.startswith(\"https://\"):\n",
        "            self.score += 0.1\n",
        "            self.explanation_parts.append(\"üîí Secure HTTPS protocol detected.\")\n",
        "        else:\n",
        "            self.explanation_parts.append(\"‚ö†Ô∏è URL uses unsecure HTTP protocol.\")\n",
        "\n",
        "    def url_length_score(self):\n",
        "        if len(self.url) < 75:\n",
        "            self.score += 0.1\n",
        "            self.explanation_parts.append(\"üìè Short, clean URL structure.\")\n",
        "        elif len(self.url) < 150:\n",
        "            self.score += 0.05\n",
        "            self.explanation_parts.append(\"üìè Moderate-length URL.\")\n",
        "        else:\n",
        "            self.explanation_parts.append(\"‚ö†Ô∏è Long or messy URL ‚Äî potential spam signal.\")\n",
        "\n",
        "    def keyword_check(self):\n",
        "        keywords = [\"study\", \"journal\", \"article\", \"research\", \"doi\"]\n",
        "        if any(word in self.url.lower() for word in keywords):\n",
        "            self.score += 0.1\n",
        "            self.explanation_parts.append(\"üîç Scholarly keywords found in URL path.\")\n",
        "\n",
        "    def spam_word_check(self):\n",
        "        spam_words = [\"shocking\", \"miracle\", \"secret\", \"click\", \"you-won‚Äôt-believe\"]\n",
        "        if any(word in self.url.lower() for word in spam_words):\n",
        "            self.score -= 0.2\n",
        "            self.explanation_parts.append(\"üö´ Spammy/clickbait keywords found ‚Äî credibility penalized.\")\n",
        "\n",
        "    def ml_model_score(self):\n",
        "        ml_score = round(random.uniform(0.6, 0.95), 2)\n",
        "        self.score += ml_score * 0.2\n",
        "        self.explanation_parts.append(f\"ü§ñ Simulated ML model credibility score = {ml_score}.\")\n",
        "\n",
        "    def serpapi_credibility_score(self):\n",
        "        if not self.serpapi_key:\n",
        "            self.explanation_parts.append(\"üîç SERP analysis skipped (no API key provided).\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            params = {\n",
        "                \"q\": self.url,\n",
        "                \"api_key\": self.serpapi_key,\n",
        "                \"num\": 5\n",
        "            }\n",
        "            search = GoogleSearch(params)\n",
        "            results = search.get_dict()\n",
        "            found = False\n",
        "\n",
        "            for idx, result in enumerate(results.get(\"organic_results\", [])):\n",
        "                serp_url = result.get(\"link\", \"\")\n",
        "                serp_domain = tldextract.extract(serp_url)\n",
        "                serp_normalized = f\"{serp_domain.domain}.{serp_domain.suffix}\"\n",
        "\n",
        "                if serp_normalized == self.domain:\n",
        "                    found = True\n",
        "                    if idx == 0:\n",
        "                        self.score += 0.2\n",
        "                        self.explanation_parts.append(\"üîù URL appears as the top Google result.\")\n",
        "                    elif idx <= 2:\n",
        "                        self.score += 0.1\n",
        "                        self.explanation_parts.append(\"‚¨ÜÔ∏è URL ranks within top 3 Google results.\")\n",
        "                    else:\n",
        "                        self.explanation_parts.append(\"üîç URL found in lower Google results.\")\n",
        "\n",
        "                    title = result.get(\"title\", \"\").lower()\n",
        "                    snippet = result.get(\"snippet\", \"\").lower()\n",
        "                    if any(kw in title + snippet for kw in [\"study\", \"doi\", \"research\", \"journal\"]):\n",
        "                        self.score += 0.1\n",
        "                        self.explanation_parts.append(\"üìö SERP snippet contains scholarly language.\")\n",
        "                    if any(kw in title for kw in [\"miracle\", \"shocking\", \"click here\"]):\n",
        "                        self.score -= 0.2\n",
        "                        self.explanation_parts.append(\"üö´ Clickbait detected in SERP title.\")\n",
        "                    break\n",
        "\n",
        "            if not found:\n",
        "                self.explanation_parts.append(\"‚ö†Ô∏è Domain not matched in top Google search results.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.explanation_parts.append(f\"‚ö†Ô∏è SERP fetch failed: {str(e)}\")\n",
        "\n",
        "    def compute_star_rating(self):\n",
        "        if self.score >= 0.9:\n",
        "            return \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "        elif self.score >= 0.75:\n",
        "            return \"‚≠ê‚≠ê‚≠ê‚≠ê\"\n",
        "        elif self.score >= 0.6:\n",
        "            return \"‚≠ê‚≠ê‚≠ê\"\n",
        "        elif self.score >= 0.4:\n",
        "            return \"‚≠ê‚≠ê\"\n",
        "        else:\n",
        "            return \"‚≠ê\"\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.domain_suffix_score()\n",
        "        self.trusted_domain_score()\n",
        "        self.recent_year_score()\n",
        "        self.https_check()\n",
        "        self.url_length_score()\n",
        "        self.keyword_check()\n",
        "        self.spam_word_check()\n",
        "        self.ml_model_score()\n",
        "        self.serpapi_credibility_score()\n",
        "\n",
        "        return {\n",
        "            \"url\": self.url,\n",
        "            \"score\": round(min(self.score, 1.0), 2),\n",
        "            \"stars\": self.compute_star_rating(),\n",
        "            \"explanation\": \" \".join(self.explanation_parts)\n",
        "        }\n",
        "\n",
        "# -------------------------------\n",
        "# üß™ User Input Test\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    from getpass import getpass\n",
        "\n",
        "    print(\"üîç Credibility Scorer\")\n",
        "    input_url = input(\"Paste a URL to evaluate: \").strip()\n",
        "    serp_key = getpass(\"üîê Enter your SerpAPI key (press Enter to skip): \").strip()\n",
        "\n",
        "    if not input_url.startswith(\"http\"):\n",
        "        print(\"‚ö†Ô∏è Please enter a valid URL with http:// or https://\")\n",
        "    else:\n",
        "        scorer = CredibilityScorer(input_url, serpapi_key=serp_key if serp_key else None)\n",
        "        result = scorer.evaluate()\n",
        "\n",
        "        print(\"\\nüéØ Evaluation Result\")\n",
        "        print(f\"üîó URL: {result['url']}\")\n",
        "        print(f\"‚≠ê Score: {result['stars']} ({result['score']} / 1.0)\")\n",
        "        print(\"üìò Explanation:\")\n",
        "        print(result[\"explanation\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6vjCFJDgxvw",
        "outputId": "767f7d8f-98c9-409f-b34a-393777f66462"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Credibility Scorer\n",
            "Paste a URL to evaluate: https://www.nature.com/articles/s41586-020-03119-7\n",
            "üîê Enter your SerpAPI key (press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "üéØ Evaluation Result\n",
            "üîó URL: https://www.nature.com/articles/s41586-020-03119-7\n",
            "‚≠ê Score: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (0.94 / 1.0)\n",
            "üìò Explanation:\n",
            "‚ö†Ô∏è '.com' is a common/commercial domain ‚Äî minimal trust boost. ‚úÖ Trusted academic publisher detected: `nature.com`. üìÖ No recent date detected in URL. üîí Secure HTTPS protocol detected. üìè Short, clean URL structure. üîç Scholarly keywords found in URL path. ü§ñ Simulated ML model credibility score = 0.68. ‚ö†Ô∏è Domain not matched in top Google search results.\n"
          ]
        }
      ]
    }
  ]
}